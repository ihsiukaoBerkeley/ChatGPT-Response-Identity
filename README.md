## Evaluating ChatGPT’s Responses to Different Identities

### Overview

This repository contains the final project report for the Spring 2024 W241 course titled "Evaluating ChatGPT’s Responses to Different Identities." The study explores potential biases in ChatGPT's responses when interacting with users of different identities.

### Authors

Jing Wen
I-Hsiu Kao
Jacob Schamp
Paul Cooper

### Abstract

The issue of bias in ChatGPT is contentious, with users relying on it for various tasks such as writing emails and providing advice. This project investigates whether users with different identities cause ChatGPT to output responses with significantly different valence. Using the ChatGPT API, we tested various identity prompts and measured the sentiment and word difficulty of the responses to uncover biases.

### References

Deshpande, A., et al. (2023). Toxicity in CHATGPT: Analyzing persona-assigned language models. Findings of the Association for Computational Linguistics: EMNLP 2023.

Motoki, F., et al. (2023). More human than human: Measuring chatgpt political bias. Public Choice, 198(1–2), 3–23.

Bird, S., et al. (2023). Natural Language Toolkit (NLTK) Documentation.

Ouyang, L., et al. (2022). Training language models to follow instructions with human feedback. ArXiv, abs/2203.02155.

Dale, E., & Chall, J. S. (1948). A formula for predicting readability: Instructions. Educational Research Bulletin, 27(2), 37-54.
